diff --git a/cluster-autoscaler/core/scaleup/orchestrator/orchestrator.go b/cluster-autoscaler/core/scaleup/orchestrator/orchestrator.go
index 7fb533570..18e8adca9 100644
--- a/cluster-autoscaler/core/scaleup/orchestrator/orchestrator.go
+++ b/cluster-autoscaler/core/scaleup/orchestrator/orchestrator.go
@@ -604,7 +604,10 @@ func (o *ScaleUpOrchestrator) SchedulablePodGroups(
 	var schedulablePodGroups []estimator.PodEquivalenceGroup
 	for _, eg := range podEquivalenceGroups {
 		samplePod := eg.Pods[0]
-		if err := o.autoscalingContext.PredicateChecker.CheckPredicates(o.autoscalingContext.ClusterSnapshot, samplePod, nodeInfo.Node().Name); err == nil {
+		err := o.autoscalingContext.PredicateChecker.CheckPredicates(o.autoscalingContext.ClusterSnapshot, samplePod, nodeInfo.Node().Name)
+		// Ignore inter-pod affinity since this is not usually a reason to fail the ASG
+		// (unless the anti-affinity conflict is with a DaemonSet pod, but there's no way to tell)
+		if err == nil || err.PredicateName() == "InterPodAffinity" {
 			// Add pods to option.
 			schedulablePodGroups = append(schedulablePodGroups, estimator.PodEquivalenceGroup{
 				Pods: eg.Pods,
diff --git a/cluster-autoscaler/utils/kubernetes/listers.go b/cluster-autoscaler/utils/kubernetes/listers.go
index b9be94b6e..5efb40df2 100644
--- a/cluster-autoscaler/utils/kubernetes/listers.go
+++ b/cluster-autoscaler/utils/kubernetes/listers.go
@@ -17,10 +17,12 @@ limitations under the License.
 package kubernetes
 
 import (
+	"encoding/json"
 	"time"
 
 	apiv1 "k8s.io/api/core/v1"
 	policyv1 "k8s.io/api/policy/v1"
+	"k8s.io/apimachinery/pkg/api/resource"
 	"k8s.io/apimachinery/pkg/fields"
 	"k8s.io/apimachinery/pkg/labels"
 	"k8s.io/client-go/informers"
@@ -46,6 +48,14 @@ type ListerRegistry interface {
 	StatefulSetLister() v1appslister.StatefulSetLister
 }
 
+// copied from github.com/neondatabase/autoscaling, neonvm/apis/neonvm/v1/virtualmachine_types.go.
+//
+// this is duplicated so we're not *also* managing an additional dependency.
+type virtualMachineUsage struct {
+	CPU    resource.Quantity `json:"cpu"`
+	Memory resource.Quantity `json:"memory"`
+}
+
 type listerRegistryImpl struct {
 	allNodeLister               NodeLister
 	readyNodeLister             NodeLister
@@ -221,6 +231,22 @@ type AllPodLister struct {
 	podLister v1lister.PodLister
 }
 
+func updatePodRequestsFromNeonVMAnnotation(pod *apiv1.Pod) {
+	annotation, ok := pod.Annotations["vm.neon.tech/usage"]
+	if !ok {
+		return
+	}
+
+	var usage virtualMachineUsage
+	if err := json.Unmarshal([]byte(annotation), &usage); err != nil {
+		return
+	}
+	pod.Spec.Containers[0].Resources.Requests = apiv1.ResourceList(map[apiv1.ResourceName]resource.Quantity{
+		apiv1.ResourceCPU:    usage.CPU,
+		apiv1.ResourceMemory: usage.Memory,
+	})
+}
+
 // List returns all scheduled pods.
 func (lister *AllPodLister) List() ([]*apiv1.Pod, error) {
 	var pods []*apiv1.Pod
@@ -231,7 +257,10 @@ func (lister *AllPodLister) List() ([]*apiv1.Pod, error) {
 	}
 	for _, p := range allPods {
 		if p.Status.Phase != apiv1.PodSucceeded && p.Status.Phase != apiv1.PodFailed {
-			pods = append(pods, p)
+			// We need to make a copy of the pod to avoid modifying the original pod, since *p is a pointer to the object in the informer cache.
+			podCopy := p.DeepCopy()
+			updatePodRequestsFromNeonVMAnnotation(podCopy)
+			pods = append(pods, podCopy)
 		}
 	}
 	return pods, nil
