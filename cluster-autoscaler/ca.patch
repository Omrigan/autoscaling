diff --git a/cluster-autoscaler/utils/kubernetes/listers.go b/cluster-autoscaler/utils/kubernetes/listers.go
index d0033550f..a08d834f2 100644
--- a/cluster-autoscaler/utils/kubernetes/listers.go
+++ b/cluster-autoscaler/utils/kubernetes/listers.go
@@ -17,14 +17,20 @@ limitations under the License.
 package kubernetes
 
 import (
+	"encoding/json"
+	"math"
 	"time"
 
 	appsv1 "k8s.io/api/apps/v1"
 	batchv1 "k8s.io/api/batch/v1"
 	apiv1 "k8s.io/api/core/v1"
 	policyv1 "k8s.io/api/policy/v1"
+	"k8s.io/apimachinery/pkg/api/resource"
+	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
 	"k8s.io/apimachinery/pkg/fields"
 	"k8s.io/apimachinery/pkg/labels"
+	"k8s.io/apimachinery/pkg/runtime"
+	"k8s.io/apimachinery/pkg/watch"
 	client "k8s.io/client-go/kubernetes"
 	v1appslister "k8s.io/client-go/listers/apps/v1"
 	v1batchlister "k8s.io/client-go/listers/batch/v1"
@@ -185,6 +191,7 @@ func NewUnschedulablePodInNamespaceLister(kubeClient client.Interface, namespace
 	selector := fields.ParseSelectorOrDie("spec.nodeName==" + "" + ",status.phase!=" +
 		string(apiv1.PodSucceeded) + ",status.phase!=" + string(apiv1.PodFailed))
 	podListWatch := cache.NewListWatchFromClient(kubeClient.CoreV1().RESTClient(), "pods", namespace, selector)
+	podListWatch = wrapListWatchWithNeonVMUsage(podListWatch)
 	store, reflector := cache.NewNamespaceKeyedIndexerAndReflector(podListWatch, &apiv1.Pod{}, time.Hour)
 	podLister := v1lister.NewPodLister(store)
 	go reflector.Run(stopchannel)
@@ -209,6 +216,7 @@ func NewScheduledPodLister(kubeClient client.Interface, stopchannel <-chan struc
 	selector := fields.ParseSelectorOrDie("spec.nodeName!=" + "" + ",status.phase!=" +
 		string(apiv1.PodSucceeded) + ",status.phase!=" + string(apiv1.PodFailed))
 	podListWatch := cache.NewListWatchFromClient(kubeClient.CoreV1().RESTClient(), "pods", apiv1.NamespaceAll, selector)
+	podListWatch = wrapListWatchWithNeonVMUsage(podListWatch)
 	store, reflector := cache.NewNamespaceKeyedIndexerAndReflector(podListWatch, &apiv1.Pod{}, time.Hour)
 	podLister := v1lister.NewPodLister(store)
 	go reflector.Run(stopchannel)
@@ -218,6 +226,137 @@ func NewScheduledPodLister(kubeClient client.Interface, stopchannel <-chan struc
 	}
 }
 
+// copied from github.com/neondatabase/autoscaling, neonvm/apis/neonvm/v1/virtualmachine_types.go.
+//
+// this is duplicated so we're not *also* managing an additional dependency.
+type virtualMachineUsage struct {
+	CPU    resource.Quantity `json:"cpu"`
+	Memory resource.Quantity `json:"memory"`
+}
+
+// copied from github.com/neondatabase/autoscaling, neonvm/apis/neonvm/v1/virtualmachine_types.go.
+//
+// this is duplicated so we're not *also* managing an additional dependency.
+type VirtualMachineOvercommitSettings struct {
+	CPU *float64 `json:"cpu,omitempty"`
+	Mem *float64 `json:"mem,omitempty"`
+}
+
+func wrapListWatchWithNeonVMUsage(lw *cache.ListWatch) *cache.ListWatch {
+	updatePodRequestsFromNeonVMAnnotation := func(pod *apiv1.Pod) {
+		usageAnnotation, ok := pod.Annotations["vm.neon.tech/usage"]
+		if !ok {
+			return
+		}
+
+		var usage virtualMachineUsage
+		if err := json.Unmarshal([]byte(usageAnnotation), &usage); err != nil {
+			return
+		}
+
+		// Handle overcommit, if present
+		overcommitAnnotation, ok := pod.Annotations["vm.neon.tech/overcommit"]
+		if ok {
+			var overcommit VirtualMachineOvercommitSettings
+			if err := json.Unmarshal([]byte(overcommitAnnotation), &overcommit); err != nil {
+				return
+			}
+
+			if cpuFactor := overcommit.CPU; cpuFactor != nil {
+				newCPU := resource.NewMilliQuantity(
+					int64(math.Round(float64(usage.CPU.MilliValue())/(*cpuFactor))),
+					usage.CPU.Format,
+				)
+				usage.CPU = *newCPU
+			}
+			if memFactor := overcommit.Mem; memFactor != nil {
+				newMem := resource.NewQuantity(
+					int64(math.Round(float64(usage.Memory.Value())/(*memFactor))),
+					usage.Memory.Format,
+				)
+				usage.Memory = *newMem
+			}
+		}
+
+		pod.Spec.Containers[0].Resources.Requests = apiv1.ResourceList(map[apiv1.ResourceName]resource.Quantity{
+			apiv1.ResourceCPU:    usage.CPU,
+			apiv1.ResourceMemory: usage.Memory,
+		})
+	}
+
+	return &cache.ListWatch{
+		ListFunc: func(options metav1.ListOptions) (runtime.Object, error) {
+			obj, err := lw.List(options)
+			if err != nil {
+				return obj, err
+			}
+
+			list := obj.(*apiv1.PodList)
+			for i := range list.Items {
+				updatePodRequestsFromNeonVMAnnotation(&list.Items[i])
+			}
+			return obj, nil
+		},
+		WatchFunc: func(options metav1.ListOptions) (watch.Interface, error) {
+			iface, err := lw.Watch(options)
+			if err != nil {
+				return iface, err
+			}
+
+			// Wrap the channel to update the pods as they come through
+			wrappedEvents := make(chan watch.Event)
+			proxyIface := watch.NewProxyWatcher(wrappedEvents)
+
+			go func() {
+				events := iface.ResultChan()
+
+				for {
+					var ok bool
+					var ev watch.Event
+
+					select {
+					case <-proxyIface.StopChan():
+						return
+					case ev, ok = <-events:
+						if !ok {
+							close(wrappedEvents)
+							return
+						}
+					}
+
+					// Quoting the docs on watch.Event.Object:
+					//
+					// > Object is:
+					// >  * If Type is Added or Modified: the new state of the object
+					// >  * If type is Deleted: the state of the object immediately before deletion.
+					// >  * If Type is Bookmark: the object [ ... ] where only ResourceVersion field
+					// >    is set.
+					// >  * If Type is Error: *api.Status is recommended; other types may make sense
+					// >    depending on context.
+					//
+					// So basically, we want to process the object only if ev.Type is Added,
+					// Modified, or Deleted.
+					if ev.Type == watch.Added || ev.Type == watch.Modified || ev.Type == watch.Deleted {
+						pod := ev.Object.(*apiv1.Pod)
+						updatePodRequestsFromNeonVMAnnotation(pod)
+					}
+
+					// Pass along the maybe-updated event
+					select {
+					case <-proxyIface.StopChan():
+						return
+					case wrappedEvents <- ev:
+						// continue on to next event
+					}
+				}
+			}()
+
+			return proxyIface, nil
+		},
+		DisableChunking: lw.DisableChunking,
+	}
+}
+
 // NodeLister lists nodes.
 type NodeLister interface {
 	List() ([]*apiv1.Node, error)
